{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3,16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(16)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc       = nn.Linear(16**3, 10)\n",
    "        self.count = 0 \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x += 100\n",
    "        if (self.count % 20 == 0) :\n",
    "            print(f'{self.count}. before BatchNorm2d : ', x.view(x.size(0),-1)[0][0:10], end='\\t')\n",
    "        x = self.bn1(x)\n",
    "        if (self.count % 20 == 0) :\n",
    "            print(f' -> {self.count}. after BatchNorm2d : ', x.view(x.size(0),-1)[0][0:10])\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc(x)\n",
    "        self.count += 1\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 7, 7, 9, 7, 8, 4, 0, 2, 3, 8, 6, 5, 4, 5, 9, 7, 8, 1, 2, 3, 0, 9, 9,\n",
      "        2, 7, 5, 5, 0, 1, 1, 6, 5, 9, 5, 5, 2, 1, 1, 3, 4, 1, 0, 2, 5, 7, 1, 5,\n",
      "        2, 0, 4, 5, 9, 8, 7, 5, 0, 2, 6, 9, 6, 8, 3, 9, 3, 4, 7, 8, 6, 5, 8, 0,\n",
      "        2, 2, 7, 3, 6, 4, 3, 5, 2, 7, 6, 3, 1, 6, 8, 1, 6, 6, 7, 1, 4, 5, 4, 5,\n",
      "        9, 0, 8, 3, 3, 3, 9, 6, 9, 1, 7, 0, 5, 6, 9, 9, 3, 8, 9, 8, 6, 6, 5, 5,\n",
      "        0, 0, 3, 7, 1, 2, 6, 1])\n",
      "0. before BatchNorm2d :  tensor([100.1686, 100.4076, 100.6059, 100.5386,  99.9092, 100.7679, 100.2677,\n",
      "        100.6640, 100.0627, 100.1144], grad_fn=<SliceBackward0>)\t -> 0. after BatchNorm2d :  tensor([-0.0264,  0.4722,  0.8859,  0.7456, -0.5677,  1.2240,  0.1802,  1.0072,\n",
      "        -0.2474, -0.1395], grad_fn=<SliceBackward0>)\n",
      "Epoch [10/100], Loss : 6.3741\n",
      "Epoch [20/100], Loss : 3.6105\n",
      "20. before BatchNorm2d :  tensor([100.1972, 100.4443, 100.5880, 100.5302,  99.9322, 100.7420, 100.2735,\n",
      "        100.6495, 100.0908, 100.1107], grad_fn=<SliceBackward0>)\t -> 20. after BatchNorm2d :  tensor([-0.0139,  0.4819,  0.7702,  0.6543, -0.5456,  1.0793,  0.1391,  0.8937,\n",
      "        -0.2275, -0.1875], grad_fn=<SliceBackward0>)\n",
      "Epoch [30/100], Loss : 0.7387\n",
      "Epoch [40/100], Loss : 0.2545\n",
      "40. before BatchNorm2d :  tensor([100.2068, 100.4568, 100.5837, 100.5252,  99.9356, 100.7373, 100.2659,\n",
      "        100.6365, 100.0976, 100.1074], grad_fn=<SliceBackward0>)\t -> 40. after BatchNorm2d :  tensor([ 3.3569e-04,  5.0615e-01,  7.6289e-01,  6.4458e-01, -5.4865e-01,\n",
      "         1.0738e+00,  1.1980e-01,  8.6987e-01, -2.2069e-01, -2.0093e-01],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Epoch [50/100], Loss : 0.1825\n",
      "Epoch [60/100], Loss : 0.1460\n",
      "60. before BatchNorm2d :  tensor([100.2092, 100.4604, 100.5838, 100.5238,  99.9356, 100.7385, 100.2612,\n",
      "        100.6304, 100.0980, 100.1050], grad_fn=<SliceBackward0>)\t -> 60. after BatchNorm2d :  tensor([ 0.0077,  0.5210,  0.7733,  0.6508, -0.5514,  1.0894,  0.1139,  0.8685,\n",
      "        -0.2195, -0.2052], grad_fn=<SliceBackward0>)\n",
      "Epoch [70/100], Loss : 0.1209\n",
      "Epoch [80/100], Loss : 0.1027\n",
      "80. before BatchNorm2d :  tensor([100.2110, 100.4629, 100.5838, 100.5231,  99.9355, 100.7388, 100.2580,\n",
      "        100.6270, 100.0985, 100.1030], grad_fn=<SliceBackward0>)\t -> 80. after BatchNorm2d :  tensor([ 0.0133,  0.5321,  0.7810,  0.6559, -0.5538,  1.1000,  0.1101,  0.8699,\n",
      "        -0.2182, -0.2089], grad_fn=<SliceBackward0>)\n",
      "Epoch [90/100], Loss : 0.0889\n",
      "Epoch [100/100], Loss : 0.0783\n"
     ]
    }
   ],
   "source": [
    "# Basic settings\n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = t.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# get some sample data with batch size of 128\n",
    "pl.seed_everything(42)\n",
    "inputs = t.randn(128, 3, 32, 32)\n",
    "labels = t.randint(0,10,(128,))\n",
    "print(labels[:])\n",
    "\n",
    "# training loop\n",
    "for epoch in range(100):\n",
    "    # forward pass \n",
    "    outputs = model(inputs)\n",
    "    loss    = criterion(outputs, labels)\n",
    "    \n",
    "    # backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1)%10 == 0 :\n",
    "        print(f\"Epoch [{epoch+1}/100], Loss : {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a80da1299cfb6fcf32c2920c6d7840566b2321111569b9b748c12c8898665689"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
